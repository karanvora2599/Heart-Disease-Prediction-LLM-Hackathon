{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = './dataset/processed_inputs.csv'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df_input, df_outputs):\n",
    "    \"\"\"\n",
    "    Example Usage:\n",
    "    ```\n",
    "    df_input = pd.read_csv('dataset/inputs.csv')\n",
    "    df_outputs = pd.read_csv('dataset/labels.csv')\n",
    "\n",
    "    df = preprocess(df_input, df_outputs)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    df = pd.merge(df_input, df_outputs, on='PatientID', how='inner')\n",
    "\n",
    "    # Which columns to keep\n",
    "    columns_to_keep = [\n",
    "        'PatientID',\n",
    "        'Sex',\n",
    "        'HIVTesting',\n",
    "        'ECigaretteUsage',\n",
    "        'DifficultyConcentrating',\n",
    "        'HadAsthma',\n",
    "        'HadDepressiveDisorder',\n",
    "        'CovidPos',\n",
    "        'FluVaxLast12',\n",
    "        'RaceEthnicityCategory',\n",
    "        'HadDiabetes',\n",
    "        'DifficultyDressingBathing',\n",
    "        'ChestScan',\n",
    "        'HadCOPD',\n",
    "        'BlindOrVisionDifficulty',\n",
    "        'HighRiskLastYear',\n",
    "        'HadAngina',\n",
    "        'PneumoVaxEver',\n",
    "        'HadSkinCancer',\n",
    "        'HadArthritis',\n",
    "        'DeafOrHardOfHearing',\n",
    "        'AlcoholDrinkers',\n",
    "        'HadKidneyDisease',\n",
    "        'TetanusLast10Tdap',\n",
    "        'SmokerStatus',\n",
    "        'HeightInMeters',\n",
    "        'BMI',\n",
    "        'HadHeartAttack'\n",
    "    ]\n",
    "    df = df[columns_to_keep]\n",
    "\n",
    "    # Turn to bool\n",
    "    columns_to_transform = [\n",
    "        'DifficultyConcentrating',\n",
    "        'HadAsthma',\n",
    "        'HadDepressiveDisorder',\n",
    "        'CovidPos',\n",
    "        'FluVaxLast12',\n",
    "        'DifficultyDressingBathing',\n",
    "        'ChestScan',\n",
    "        'HadCOPD',\n",
    "        'BlindOrVisionDifficulty',\n",
    "        'HighRiskLastYear',\n",
    "        'HadAngina',\n",
    "        'PneumoVaxEver',\n",
    "        'HadSkinCancer',\n",
    "        'HadArthritis',\n",
    "        'DeafOrHardOfHearing',\n",
    "        'AlcoholDrinkers',\n",
    "        'HadKidneyDisease',\n",
    "        'HadHeartAttack'\n",
    "    ]\n",
    "    df[columns_to_transform] = df[columns_to_transform].astype(bool)\n",
    "\n",
    "    # Rounding\n",
    "    df['BMI'] = df['BMI'].round(2)\n",
    "    df['HeightInMeters'] = df['HeightInMeters'].round(2)\n",
    "\n",
    "    ### Fix Column Names\n",
    "    new_columns = ['Patient ID', 'Sex', 'HIV Testing', 'E-Cigarette Usage',\n",
    "               'Difficulty Concentrating', 'Had Asthma', 'Had Depressive Disorder',\n",
    "               'Covid Positive', 'Flu Vaccine Last 12 Months', 'Race/Ethnicity Category', 'Had Diabetes',\n",
    "               'Difficulty Dressing/Bathing', 'Chest Scan', 'Had COPD',\n",
    "               'Blind or Vision Difficulty', 'High Risk Last Year', 'Had Angina',\n",
    "               'Pneumonia Vaccine Ever', 'Had Skin Cancer', 'Had Arthritis', 'Deaf or Hard of Hearing',\n",
    "               'Alcohol Drinkers', 'Had Kidney Disease', 'Tetanus Last 10 Years (Tdap)',\n",
    "               'Smoker Status', 'Height in Meters', 'BMI', 'Had Heart Attack']\n",
    "\n",
    "    df.columns = new_columns\n",
    "\n",
    "    return df\n",
    "\n",
    "df_input =  pd.read_csv('./dataset/inputs.csv')\n",
    "df_outputs = pd.read_csv('./dataset/labels.csv')\n",
    "df = preprocess(df_input, df_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_string(row):\n",
    "    # row is a row in dataframe\n",
    "    feature1 = \", \".join([f\"{col}: {row[col]}\" for col in row.keys()])\n",
    "    return feature1\n",
    "\n",
    "def format_string_ALL(df):\n",
    "    class_0 = df[df['Had Heart Attack'] == 0]\n",
    "    class_1 = df[df['Had Heart Attack'] == 1]\n",
    "\n",
    "    class_0 = class_0.sample(frac=1)\n",
    "    class_1 = class_1.sample(frac=1)\n",
    "\n",
    "    class_0 = class_0.drop(columns=[\"Patient ID\"])\n",
    "    class_1 = class_1.drop(columns=[\"Patient ID\"])\n",
    "\n",
    "    # class_0 and 1 are pandas df\n",
    "    # Generate features for class 0 and class 1\n",
    "    feature1 = [format_string(row) for _, row in class_0.sample(frac = 1).iterrows()]\n",
    "    feature2 = [format_string(row) for _, row in class_1.sample(frac = 1).iterrows()]\n",
    "\n",
    "    return feature1, feature2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature0, feature1 = format_string_ALL(df)\n",
    "\n",
    "train_index_0 = round(len(feature0)/5)\n",
    "train_index_1 = round(len(feature1)/5)\n",
    "test_0 = feature0[:train_index_0]\n",
    "test_1 = feature1[:train_index_1]\n",
    "train_0 = feature0[train_index_0:]\n",
    "train_1 = feature1[train_index_1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from typing import List, Tuple\n",
    "\n",
    "class BatchGenerator:\n",
    "    def __init__(self, long_list: List, short_list: List, long_label: List, short_label: List):\n",
    "        self.short_original = short_list\n",
    "        self.long_original = long_list\n",
    "        self.short_cycle = list(short_list)\n",
    "        self.long_cycle = list(long_list)\n",
    "        self.long_label = long_label\n",
    "        self.short_label = short_label\n",
    "        \n",
    "    def get_batch(self, batch_size: int) -> List[Tuple]:\n",
    "        batch = []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            # Replenish and shuffle if needed\n",
    "            if not self.short_cycle:\n",
    "                self.short_cycle = list(self.short_original)\n",
    "                shuffle(self.short_cycle)\n",
    "                \n",
    "            if not self.long_cycle:\n",
    "                self.long_cycle = list(self.long_original)\n",
    "                shuffle(self.long_cycle)\n",
    "                \n",
    "            # Get next items\n",
    "            batch.append((self.long_cycle.pop(0), self.short_cycle.pop(0)))\n",
    "        \n",
    "        batch = list(zip(*batch))\n",
    "        return list(batch[0]+batch[1]), self.long_label[:batch_size] + self.short_label[:batch_size]\n",
    "\n",
    "generator = BatchGenerator(train_0, train_1, [0] * len(train_0), [1] * len(train_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/xj2173/Heart-Disease-Prediction-LLM-Hackathon/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import InputExample, losses\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Transformer = SentenceTransformer('multi-qa-mpnet-base-dot-v1').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Regression, self).__init__()\n",
    "        self.linear = nn.LazyLinear(1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))\n",
    "linear_model = Regression().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.7388517260551453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/xj2173/Heart-Disease-Prediction-LLM-Hackathon/.venv/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision is 0.8919146038642977, recall is 0.9444123060741519, f1 is 0.9174130415427268\n",
      "loss is 0.7675619125366211\n",
      "loss is 0.7199819087982178\n",
      "loss is 0.6950812339782715\n",
      "loss is 0.6932196617126465\n",
      "loss is 0.6931595802307129\n",
      "loss is 0.6931620240211487\n",
      "loss is 0.693181037902832\n",
      "loss is 0.6931676864624023\n",
      "loss is 0.6931626796722412\n",
      "loss is 0.6931648254394531\n",
      "loss is 0.6931618452072144\n",
      "loss is 0.6931555271148682\n",
      "loss is 0.6931520700454712\n",
      "loss is 0.6931489109992981\n",
      "loss is 0.6931471824645996\n",
      "loss is 0.6931443810462952\n",
      "loss is 0.6931443810462952\n",
      "loss is 0.6931434273719788\n",
      "loss is 0.6931424736976624\n",
      "loss is 0.6931437253952026\n",
      "loss is 0.6931424140930176\n",
      "loss is 0.6931411027908325\n",
      "loss is 0.6931405067443848\n",
      "loss is 0.693141758441925\n",
      "loss is 0.6931397318840027\n",
      "loss is 0.6931393146514893\n",
      "loss is 0.6931390166282654\n",
      "loss is 0.6931376457214355\n",
      "loss is 0.6931350827217102\n",
      "loss is 0.693134069442749\n",
      "loss is 0.6931362152099609\n",
      "loss is 0.6931333541870117\n",
      "loss is 0.6931278109550476\n",
      "loss is 0.6931300163269043\n",
      "loss is 0.6931279897689819\n",
      "loss is 0.6931251287460327\n",
      "loss is 0.6931146383285522\n",
      "loss is 0.693117618560791\n",
      "loss is 0.6931059956550598\n",
      "loss is 0.6930917501449585\n",
      "loss is 0.6930950880050659\n",
      "loss is 0.6930688619613647\n",
      "loss is 0.6930727958679199\n",
      "loss is 0.6930473446846008\n",
      "loss is 0.6930170059204102\n",
      "loss is 0.6929519176483154\n",
      "loss is 0.6928619146347046\n",
      "loss is 0.6926907300949097\n",
      "loss is 0.6926041841506958\n",
      "loss is 0.6923328638076782\n",
      "precision is 0.94677808543305, recall is 0.7268472258743097, f1 is 0.8007591179840738\n",
      "loss is 0.6916675567626953\n",
      "loss is 0.6901251077651978\n",
      "loss is 0.684363067150116\n",
      "loss is 0.6758811473846436\n",
      "loss is 0.7026201486587524\n",
      "loss is 0.6935999393463135\n",
      "loss is 0.6935681104660034\n",
      "loss is 0.6936204433441162\n",
      "loss is 0.693152904510498\n",
      "loss is 0.6930965185165405\n",
      "loss is 0.6930798292160034\n",
      "loss is 0.6930209994316101\n",
      "loss is 0.6929683685302734\n",
      "loss is 0.692818820476532\n",
      "loss is 0.6927589178085327\n",
      "loss is 0.6927345991134644\n",
      "loss is 0.6917209625244141\n",
      "loss is 0.692468523979187\n",
      "loss is 0.6943694353103638\n",
      "loss is 0.6941578388214111\n",
      "loss is 0.694762110710144\n",
      "loss is 0.6864029169082642\n",
      "loss is 0.6728421449661255\n",
      "loss is 0.6100181341171265\n",
      "loss is 0.41910800337791443\n",
      "loss is 0.20811346173286438\n",
      "loss is 0.10144920647144318\n",
      "loss is 0.05627530440688133\n",
      "loss is 0.035175010561943054\n",
      "loss is 0.024429980665445328\n",
      "loss is 0.018419181928038597\n",
      "loss is 0.014836792834103107\n",
      "loss is 0.012603826820850372\n",
      "loss is 0.01115222368389368\n",
      "loss is 0.010156959295272827\n",
      "loss is 0.0094273267313838\n",
      "loss is 0.008853394538164139\n",
      "loss is 0.008373811841011047\n",
      "loss is 0.007954594679176807\n",
      "loss is 0.00757724279537797\n",
      "loss is 0.007231524214148521\n",
      "loss is 0.006912031676620245\n",
      "loss is 0.006615474820137024\n",
      "loss is 0.006339960731565952\n",
      "loss is 0.006084036082029343\n",
      "loss is 0.005846393294632435\n",
      "loss is 0.005625845864415169\n",
      "loss is 0.005421233829110861\n",
      "loss is 0.005231404211372137\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_regression = torch.optim.SGD(linear_model.parameters(), lr=0.05)\n",
    "optimizer_sentence = torch.optim.Adam(model_Transformer.parameters(), lr=0.0001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    data, label = generator.get_batch(32)\n",
    "    label = torch.tensor(label).to(device).unsqueeze(1).type(torch.float32)\n",
    "    \n",
    "    tokenized = model_Transformer.tokenize(data)\n",
    "    tokenized['input_ids'] = tokenized['input_ids'].to(device)\n",
    "    tokenized['attention_mask'] = tokenized['attention_mask'].to(device)\n",
    "\n",
    "    embedding = model_Transformer(tokenized)['sentence_embedding']\n",
    "\n",
    "    pred = linear_model(embedding)\n",
    "\n",
    "    loss = criterion(pred,label)\n",
    "    print(f'loss is {loss.item()}')\n",
    "\n",
    "    optimizer_regression.zero_grad()\n",
    "    optimizer_sentence.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_regression.step()\n",
    "    optimizer_sentence.step()\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        with torch.no_grad():\n",
    "            embedding = torch.tensor(model_Transformer.encode(test_0 + test_1)).to(device)\n",
    "            pred = linear_model(embedding).detach().cpu().numpy()\n",
    "            pred = np.round(pred)\n",
    "            precision, recall, f1_score, _ = precision_recall_fscore_support([0]*len(test_0) + [1]*len(test_1), pred, average='weighted')\n",
    "            print(f'precision is {precision}, recall is {recall}, f1 is {f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision is 1.0, recall is 1.0, f1 is 1.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    embedding = torch.tensor(model_Transformer.encode(test_0 + test_1)).to(device)\n",
    "    pred = linear_model(embedding).detach().cpu().numpy()\n",
    "    pred = np.round(pred)\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support([0]*len(test_0) + [1]*len(test_1), pred, average='weighted')\n",
    "    print(f'precision is {precision}, recall is {recall}, f1 is {f1_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Logistic Regression Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "# features, labels = batch(1000)\n",
    "# X = model_Transformer.encode(features)\n",
    "# y = np.array(labels)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.72      0.73       198\n",
      "         1.0       0.73      0.74      0.73       202\n",
      "\n",
      "    accuracy                           0.73       400\n",
      "   macro avg       0.73      0.73      0.73       400\n",
      "weighted avg       0.73      0.73      0.73       400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/xj2173/Heart-Disease-Prediction-LLM-Hackathon/.venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets\n",
    "# features, labels = batch(2000)\n",
    "# X_train, y_train = model_Transformer.encode(features), np.array(labels)  \n",
    "# X_test,  y_test  = model_Transformer.encode()\n",
    "# \n",
    "# \n",
    "# # Create and train the logistic regression model\n",
    "# model = LogisticRegression(penalty=None)\n",
    "# model.fit(X_train, y_train)\n",
    "# \n",
    "# # Make predictions\n",
    "# y_pred = model.predict(X_test)\n",
    "# \n",
    "# # Evaluate the model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy: {accuracy:.2f}\")\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
