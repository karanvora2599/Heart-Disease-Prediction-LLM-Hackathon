{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = './dataset/processed_inputs.csv'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df_input, df_outputs):\n",
    "    \"\"\"\n",
    "    Example Usage:\n",
    "    ```\n",
    "    df_input = pd.read_csv('dataset/inputs.csv')\n",
    "    df_outputs = pd.read_csv('dataset/labels.csv')\n",
    "\n",
    "    df = preprocess(df_input, df_outputs)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    df = pd.merge(df_input, df_outputs, on='PatientID', how='inner')\n",
    "\n",
    "    # Which columns to keep\n",
    "    columns_to_keep = [\n",
    "        'PatientID',\n",
    "        'Sex',\n",
    "        'HIVTesting',\n",
    "        'ECigaretteUsage',\n",
    "        'DifficultyConcentrating',\n",
    "        'HadAsthma',\n",
    "        'HadDepressiveDisorder',\n",
    "        'CovidPos',\n",
    "        'FluVaxLast12',\n",
    "        'RaceEthnicityCategory',\n",
    "        'HadDiabetes',\n",
    "        'DifficultyDressingBathing',\n",
    "        'ChestScan',\n",
    "        'HadCOPD',\n",
    "        'BlindOrVisionDifficulty',\n",
    "        'HighRiskLastYear',\n",
    "        'HadAngina',\n",
    "        'PneumoVaxEver',\n",
    "        'HadSkinCancer',\n",
    "        'HadArthritis',\n",
    "        'DeafOrHardOfHearing',\n",
    "        'AlcoholDrinkers',\n",
    "        'HadKidneyDisease',\n",
    "        'TetanusLast10Tdap',\n",
    "        'SmokerStatus',\n",
    "        'HeightInMeters',\n",
    "        'BMI',\n",
    "        'HadHeartAttack'\n",
    "    ]\n",
    "    df = df[columns_to_keep]\n",
    "\n",
    "    # Turn to bool\n",
    "    columns_to_transform = [\n",
    "        'DifficultyConcentrating',\n",
    "        'HadAsthma',\n",
    "        'HadDepressiveDisorder',\n",
    "        'CovidPos',\n",
    "        'FluVaxLast12',\n",
    "        'DifficultyDressingBathing',\n",
    "        'ChestScan',\n",
    "        'HadCOPD',\n",
    "        'BlindOrVisionDifficulty',\n",
    "        'HighRiskLastYear',\n",
    "        'HadAngina',\n",
    "        'PneumoVaxEver',\n",
    "        'HadSkinCancer',\n",
    "        'HadArthritis',\n",
    "        'DeafOrHardOfHearing',\n",
    "        'AlcoholDrinkers',\n",
    "        'HadKidneyDisease',\n",
    "        'HadHeartAttack'\n",
    "    ]\n",
    "    df[columns_to_transform] = df[columns_to_transform].astype(bool)\n",
    "\n",
    "    # Rounding\n",
    "    df['BMI'] = df['BMI'].round(2)\n",
    "    df['HeightInMeters'] = df['HeightInMeters'].round(2)\n",
    "\n",
    "    ### Fix Column Names\n",
    "    new_columns = ['Patient ID', 'Sex', 'HIV Testing', 'E-Cigarette Usage',\n",
    "               'Difficulty Concentrating', 'Had Asthma', 'Had Depressive Disorder',\n",
    "               'Covid Positive', 'Flu Vaccine Last 12 Months', 'Race/Ethnicity Category', 'Had Diabetes',\n",
    "               'Difficulty Dressing/Bathing', 'Chest Scan', 'Had COPD',\n",
    "               'Blind or Vision Difficulty', 'High Risk Last Year', 'Had Angina',\n",
    "               'Pneumonia Vaccine Ever', 'Had Skin Cancer', 'Had Arthritis', 'Deaf or Hard of Hearing',\n",
    "               'Alcohol Drinkers', 'Had Kidney Disease', 'Tetanus Last 10 Years (Tdap)',\n",
    "               'Smoker Status', 'Height in Meters', 'BMI', 'Had Heart Attack']\n",
    "\n",
    "    df.columns = new_columns\n",
    "\n",
    "    return df\n",
    "\n",
    "df_input =  pd.read_csv('./dataset/inputs.csv')\n",
    "df_outputs = pd.read_csv('./dataset/labels.csv')\n",
    "df = preprocess(df_input, df_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_string(row):\n",
    "    # row is a row in dataframe\n",
    "    feature1 = \", \".join([f\"{col}: {row[col]}\" for col in row.keys()])\n",
    "    return feature1\n",
    "\n",
    "def format_string_ALL(df):\n",
    "    class_0 = df[df['Had Heart Attack'] == 0]\n",
    "    class_1 = df[df['Had Heart Attack'] == 1]\n",
    "\n",
    "    class_0 = class_0.sample(frac=1)\n",
    "    class_1 = class_1.sample(frac=1)\n",
    "\n",
    "    class_0 = class_0.drop(columns=[\"Patient ID\"])\n",
    "    class_1 = class_1.drop(columns=[\"Patient ID\"])\n",
    "\n",
    "    # class_0 and 1 are pandas df\n",
    "    # Generate features for class 0 and class 1\n",
    "    feature1 = [format_string(row) for _, row in class_0.sample(frac = 1).iterrows()]\n",
    "    feature2 = [format_string(row) for _, row in class_1.sample(frac = 1).iterrows()]\n",
    "\n",
    "    return feature1, feature2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature0, feature1 = format_string_ALL(df)\n",
    "\n",
    "train_index_0 = round(len(feature0)/10)\n",
    "train_index_1 = round(len(feature1)/10)\n",
    "test_0 = feature0[:train_index_0]\n",
    "test_1 = feature1[:train_index_1]\n",
    "train_0 = feature0[train_index_0:]\n",
    "train_1 = feature1[train_index_1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161627"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = model_Transformer.tokenizer(train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=203, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetTokenizerFast(name_or_path='sentence-transformers/multi-qa-mpnet-base-dot-v1', vocab_size=30527, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t104: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30526: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_Transformer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorDataset, DataLoader\n\u001b[0;32m----> 3\u001b[0m dataset_0 \u001b[38;5;241m=\u001b[39m \u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m loader_0 \u001b[38;5;241m=\u001b[39m DataLoader(dataset_0, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/scratch/xj2173/Heart-Disease-Prediction-LLM-Hackathon/.venv/lib64/python3.9/site-packages/torch/utils/data/dataset.py:205\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "File \u001b[0;32m/scratch/xj2173/Heart-Disease-Prediction-LLM-Hackathon/.venv/lib64/python3.9/site-packages/torch/utils/data/dataset.py:206\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m--> 206\u001b[0m         \u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors\n\u001b[1;32m    207\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset_0 = TensorDataset([(x, y) for x, y in zip(train_0, [0]*len(train_0))])\n",
    "loader_0 = DataLoader(dataset_0, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import InputExample, losses\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Transformer = SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Logistic Regression Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "features, labels = batch(1000)\n",
    "X = model_Transformer.encode(features)\n",
    "y = np.array(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.72      0.73       198\n",
      "         1.0       0.73      0.74      0.73       202\n",
      "\n",
      "    accuracy                           0.73       400\n",
      "   macro avg       0.73      0.73      0.73       400\n",
      "weighted avg       0.73      0.73      0.73       400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/xj2173/Heart-Disease-Prediction-LLM-Hackathon/.venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets\n",
    "features, labels = batch(2000)\n",
    "X_train, y_train = model_Transformer.encode(features), np.array(labels)  \n",
    "X_test,  y_test  = model_Transformer.encode()\n",
    "\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "model = LogisticRegression(penalty=None)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
