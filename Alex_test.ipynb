{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = './dataset/processed_inputs.csv'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df_input, df_outputs):\n",
    "    \"\"\"\n",
    "    Example Usage:\n",
    "    ```\n",
    "    df_input = pd.read_csv('dataset/inputs.csv')\n",
    "    df_outputs = pd.read_csv('dataset/labels.csv')\n",
    "\n",
    "    df = preprocess(df_input, df_outputs)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    df = pd.merge(df_input, df_outputs, on='PatientID', how='inner')\n",
    "\n",
    "    # Which columns to keep\n",
    "    columns_to_keep = [\n",
    "        'PatientID',\n",
    "        'Sex',\n",
    "        'HIVTesting',\n",
    "        'ECigaretteUsage',\n",
    "        'DifficultyConcentrating',\n",
    "        'HadAsthma',\n",
    "        'HadDepressiveDisorder',\n",
    "        'CovidPos',\n",
    "        'FluVaxLast12',\n",
    "        'RaceEthnicityCategory',\n",
    "        'HadDiabetes',\n",
    "        'DifficultyDressingBathing',\n",
    "        'ChestScan',\n",
    "        'HadCOPD',\n",
    "        'BlindOrVisionDifficulty',\n",
    "        'HighRiskLastYear',\n",
    "        'HadAngina',\n",
    "        'PneumoVaxEver',\n",
    "        'HadSkinCancer',\n",
    "        'HadArthritis',\n",
    "        'DeafOrHardOfHearing',\n",
    "        'AlcoholDrinkers',\n",
    "        'HadKidneyDisease',\n",
    "        'TetanusLast10Tdap',\n",
    "        'SmokerStatus',\n",
    "        'HeightInMeters',\n",
    "        'BMI',\n",
    "        'HadHeartAttack'\n",
    "    ]\n",
    "    df = df[columns_to_keep]\n",
    "\n",
    "    # Turn to bool\n",
    "    columns_to_transform = [\n",
    "        'DifficultyConcentrating',\n",
    "        'HadAsthma',\n",
    "        'HadDepressiveDisorder',\n",
    "        'CovidPos',\n",
    "        'FluVaxLast12',\n",
    "        'DifficultyDressingBathing',\n",
    "        'ChestScan',\n",
    "        'HadCOPD',\n",
    "        'BlindOrVisionDifficulty',\n",
    "        'HighRiskLastYear',\n",
    "        'HadAngina',\n",
    "        'PneumoVaxEver',\n",
    "        'HadSkinCancer',\n",
    "        'HadArthritis',\n",
    "        'DeafOrHardOfHearing',\n",
    "        'AlcoholDrinkers',\n",
    "        'HadKidneyDisease',\n",
    "        'HadHeartAttack'\n",
    "    ]\n",
    "    df[columns_to_transform] = df[columns_to_transform].astype(bool)\n",
    "\n",
    "    # Rounding\n",
    "    df['BMI'] = df['BMI'].round(2)\n",
    "    df['HeightInMeters'] = df['HeightInMeters'].round(2)\n",
    "\n",
    "    ### Fix Column Names\n",
    "    new_columns = ['Patient ID', 'Sex', 'HIV Testing', 'E-Cigarette Usage',\n",
    "               'Difficulty Concentrating', 'Had Asthma', 'Had Depressive Disorder',\n",
    "               'Covid Positive', 'Flu Vaccine Last 12 Months', 'Race/Ethnicity Category', 'Had Diabetes',\n",
    "               'Difficulty Dressing/Bathing', 'Chest Scan', 'Had COPD',\n",
    "               'Blind or Vision Difficulty', 'High Risk Last Year', 'Had Angina',\n",
    "               'Pneumonia Vaccine Ever', 'Had Skin Cancer', 'Had Arthritis', 'Deaf or Hard of Hearing',\n",
    "               'Alcohol Drinkers', 'Had Kidney Disease', 'Tetanus Last 10 Years (Tdap)',\n",
    "               'Smoker Status', 'Height in Meters', 'BMI', 'Had Heart Attack']\n",
    "\n",
    "    df.columns = new_columns\n",
    "\n",
    "    return df\n",
    "\n",
    "df_input =  pd.read_csv('./dataset/inputs.csv')\n",
    "df_outputs = pd.read_csv('./dataset/labels.csv')\n",
    "df = preprocess(df_input, df_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_string(row):\n",
    "    # row is a row in dataframe\n",
    "    feature1 = \", \".join([f\"{col}: {row[col]}\" for col in row.keys()])\n",
    "    return feature1\n",
    "\n",
    "def format_string_ALL(df):\n",
    "    class_0 = df[df['Had Heart Attack'] == 0]\n",
    "    class_1 = df[df['Had Heart Attack'] == 1]\n",
    "\n",
    "    class_0 = class_0.sample(frac=1)\n",
    "    class_1 = class_1.sample(frac=1)\n",
    "\n",
    "    class_0 = class_0.drop(columns=[\"Patient ID\"])\n",
    "    class_1 = class_1.drop(columns=[\"Patient ID\"])\n",
    "\n",
    "    # class_0 and 1 are pandas df\n",
    "    # Generate features for class 0 and class 1\n",
    "    feature1 = [format_string(row) for _, row in class_0.sample(frac = 1).iterrows()]\n",
    "    feature2 = [format_string(row) for _, row in class_1.sample(frac = 1).iterrows()]\n",
    "\n",
    "    return feature1, feature2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature0, feature1 = format_string_ALL(df)\n",
    "\n",
    "train_index_0 = round(len(feature0)/10)\n",
    "train_index_1 = round(len(feature1)/10)\n",
    "test_0 = feature0[:train_index_0]\n",
    "test_1 = feature1[:train_index_1]\n",
    "train_0 = feature0[train_index_0:]\n",
    "train_1 = feature1[train_index_1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "List1 Chunk: ['Element1_1', 'Element1_2', 'Element1_3', 'Element1_4', 'Element1_5', 'Element1_6', 'Element1_7', 'Element1_8', 'Element1_9', 'Element1_10']\n",
      "List2 Chunk: ['Element2_1', 'Element2_2', 'Element2_3', 'Element2_4', 'Element2_5', 'Element2_6', 'Element2_7', 'Element2_8', 'Element2_9', 'Element2_10']\n",
      "\n",
      "Iteration 2:\n",
      "List1 Chunk: ['Element1_11', 'Element1_12', 'Element1_13', 'Element1_14', 'Element1_15', 'Element1_16', 'Element1_17', 'Element1_18', 'Element1_19', 'Element1_20']\n",
      "List2 Chunk: ['Element2_11', 'Element2_12', 'Element2_13', 'Element2_14', 'Element2_15', 'Element2_16', 'Element2_17', 'Element2_18', 'Element2_19', 'Element2_20']\n",
      "\n",
      "Iteration 3:\n",
      "List1 Chunk: ['Element1_21', 'Element1_22', 'Element1_23', 'Element1_24', 'Element1_25', 'Element1_26', 'Element1_27', 'Element1_28', 'Element1_29', 'Element1_30']\n",
      "List2 Chunk: ['Element2_1', 'Element2_2', 'Element2_3', 'Element2_4', 'Element2_5', 'Element2_6', 'Element2_7', 'Element2_8', 'Element2_9', 'Element2_10']\n",
      "\n",
      "Iteration 4:\n",
      "List1 Chunk: ['Element1_31', 'Element1_32', 'Element1_33', 'Element1_34', 'Element1_35', 'Element1_36', 'Element1_37', 'Element1_38', 'Element1_39', 'Element1_40']\n",
      "List2 Chunk: ['Element2_11', 'Element2_12', 'Element2_13', 'Element2_14', 'Element2_15', 'Element2_16', 'Element2_17', 'Element2_18', 'Element2_19', 'Element2_20']\n",
      "\n",
      "Iteration 5:\n",
      "List1 Chunk: ['Element1_41', 'Element1_42', 'Element1_43', 'Element1_44', 'Element1_45', 'Element1_46', 'Element1_47', 'Element1_48', 'Element1_49', 'Element1_50']\n",
      "List2 Chunk: ['Element2_1', 'Element2_2', 'Element2_3', 'Element2_4', 'Element2_5', 'Element2_6', 'Element2_7', 'Element2_8', 'Element2_9', 'Element2_10']\n",
      "\n",
      "Iteration 6:\n",
      "List1 Chunk: ['Element1_51', 'Element1_52', 'Element1_53', 'Element1_54', 'Element1_55', 'Element1_56', 'Element1_57', 'Element1_58', 'Element1_59', 'Element1_60']\n",
      "List2 Chunk: ['Element2_11', 'Element2_12', 'Element2_13', 'Element2_14', 'Element2_15', 'Element2_16', 'Element2_17', 'Element2_18', 'Element2_19', 'Element2_20']\n",
      "\n",
      "Iteration 7:\n",
      "List1 Chunk: ['Element1_61', 'Element1_62', 'Element1_63', 'Element1_64', 'Element1_65', 'Element1_66', 'Element1_67', 'Element1_68', 'Element1_69', 'Element1_70']\n",
      "List2 Chunk: ['Element2_1', 'Element2_2', 'Element2_3', 'Element2_4', 'Element2_5', 'Element2_6', 'Element2_7', 'Element2_8', 'Element2_9', 'Element2_10']\n",
      "\n",
      "Iteration 8:\n",
      "List1 Chunk: ['Element1_71', 'Element1_72', 'Element1_73', 'Element1_74', 'Element1_75', 'Element1_76', 'Element1_77', 'Element1_78', 'Element1_79', 'Element1_80']\n",
      "List2 Chunk: ['Element2_11', 'Element2_12', 'Element2_13', 'Element2_14', 'Element2_15', 'Element2_16', 'Element2_17', 'Element2_18', 'Element2_19', 'Element2_20']\n",
      "\n",
      "Iteration 9:\n",
      "List1 Chunk: ['Element1_81', 'Element1_82', 'Element1_83', 'Element1_84', 'Element1_85', 'Element1_86', 'Element1_87', 'Element1_88', 'Element1_89', 'Element1_90']\n",
      "List2 Chunk: ['Element2_1', 'Element2_2', 'Element2_3', 'Element2_4', 'Element2_5', 'Element2_6', 'Element2_7', 'Element2_8', 'Element2_9', 'Element2_10']\n",
      "\n",
      "Iteration 10:\n",
      "List1 Chunk: ['Element1_91', 'Element1_92', 'Element1_93', 'Element1_94', 'Element1_95', 'Element1_96', 'Element1_97', 'Element1_98', 'Element1_99', 'Element1_100']\n",
      "List2 Chunk: ['Element2_11', 'Element2_12', 'Element2_13', 'Element2_14', 'Element2_15', 'Element2_16', 'Element2_17', 'Element2_18', 'Element2_19', 'Element2_20']\n",
      "\n",
      "Iteration 11:\n",
      "List1 Chunk: ['Element1_101', 'Element1_102', 'Element1_103', 'Element1_104', 'Element1_105', 'Element1_106', 'Element1_107', 'Element1_108', 'Element1_109', 'Element1_110']\n",
      "List2 Chunk: ['Element2_1', 'Element2_2', 'Element2_3', 'Element2_4', 'Element2_5', 'Element2_6', 'Element2_7', 'Element2_8', 'Element2_9', 'Element2_10']\n",
      "\n",
      "Iteration 12:\n",
      "List1 Chunk: ['Element1_111', 'Element1_112', 'Element1_113', 'Element1_114', 'Element1_115', 'Element1_116', 'Element1_117', 'Element1_118', 'Element1_119', 'Element1_120']\n",
      "List2 Chunk: ['Element2_11', 'Element2_12', 'Element2_13', 'Element2_14', 'Element2_15', 'Element2_16', 'Element2_17', 'Element2_18', 'Element2_19', 'Element2_20']\n",
      "\n",
      "Iteration 13:\n",
      "List1 Chunk: ['Element1_121', 'Element1_122', 'Element1_123', 'Element1_124', 'Element1_125', 'Element1_126', 'Element1_127']\n",
      "List2 Chunk: ['Element2_1', 'Element2_2', 'Element2_3', 'Element2_4', 'Element2_5', 'Element2_6', 'Element2_7', 'Element2_8', 'Element2_9', 'Element2_10']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "def chunked_iterable(iterable, chunk_size):\n",
    "    \"\"\"\n",
    "    Yield successive chunks of size `chunk_size` from `iterable`.\n",
    "    \n",
    "    Args:\n",
    "        iterable (list): The list to be divided into chunks.\n",
    "        chunk_size (int): The size of each chunk.\n",
    "    \n",
    "    Yields:\n",
    "        list: Chunks of the original list.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(iterable), chunk_size):\n",
    "        yield iterable[i:i + chunk_size]\n",
    "\n",
    "# Sample data\n",
    "list1 = [f\"Element1_{i}\" for i in range(1, 128)]  # Larger list with 100 elements\n",
    "list2 = [f\"Element2_{i}\" for i in range(1, 21)]   # Smaller list with 20 elements\n",
    "\n",
    "chunk_size = 10  # Define the size of each chunk\n",
    "\n",
    "# Create chunked iterators\n",
    "list1_chunks = chunked_iterable(list1, chunk_size)\n",
    "list2_chunks = list(chunked_iterable(list2, chunk_size))  # Convert to list for cycling\n",
    "\n",
    "# Create an infinite cycling iterator for list2 chunks\n",
    "list2_cycle = itertools.cycle(list2_chunks)\n",
    "\n",
    "# Iterate through list1 and list2 in chunks\n",
    "for idx, chunk1 in enumerate(list1_chunks, 1):\n",
    "    chunk2 = next(list2_cycle)\n",
    "    \n",
    "    # Example comparison: Print both chunks\n",
    "    print(f\"Iteration {idx}:\")\n",
    "    print(f\"List1 Chunk: {chunk1}\")\n",
    "    print(f\"List2 Chunk: {chunk2}\\n\")\n",
    "    \n",
    "    # Replace the above print statements with your comparison logic\n",
    "    # For example:\n",
    "    # comparison_result = compare_chunks(chunk1, chunk2)\n",
    "    # process_result(comparison_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "List1 Chunk (10): ['Element1_1', 'Element1_2', 'Element1_3', 'Element1_4', 'Element1_5', 'Element1_6', 'Element1_7', 'Element1_8', 'Element1_9', 'Element1_10']\n",
      "List2 Chunk (10): ['Element2_1', 'Element2_2', 'Element2_3', 'Element2_4', 'Element2_5', 'Element2_6', 'Element2_7', 'Element2_8', 'Element2_9', 'Element2_10']\n",
      "Common Elements: set()\n",
      "\n",
      "Iteration 2:\n",
      "List1 Chunk (10): ['Element1_11', 'Element1_12', 'Element1_13', 'Element1_14', 'Element1_15', 'Element1_16', 'Element1_17', 'Element1_18', 'Element1_19', 'Element1_20']\n",
      "List2 Chunk (10): ['Element2_11', 'Element2_12', 'Element2_13', 'Element2_14', 'Element2_15', 'Element2_16', 'Element2_17', 'Element2_18', 'Element2_19', 'Element2_20']\n",
      "Common Elements: set()\n",
      "\n",
      "Iteration 3:\n",
      "List1 Chunk (10): ['Element1_21', 'Element1_22', 'Element1_23', 'Element1_24', 'Element1_25', 'Element1_26', 'Element1_27', 'Element1_28', 'Element1_29', 'Element1_30']\n",
      "List2 Chunk (10): ['Element2_1', 'Element2_2', 'Element2_3', 'Element2_4', 'Element2_5', 'Element2_6', 'Element2_7', 'Element2_8', 'Element2_9', 'Element2_10']\n",
      "Common Elements: set()\n",
      "\n",
      "Iteration 4:\n",
      "List1 Chunk (10): ['Element1_31', 'Element1_32', 'Element1_33', 'Element1_34', 'Element1_35', 'Element1_36', 'Element1_37', 'Element1_38', 'Element1_39', 'Element1_40']\n",
      "List2 Chunk (10): ['Element2_11', 'Element2_12', 'Element2_13', 'Element2_14', 'Element2_15', 'Element2_16', 'Element2_17', 'Element2_18', 'Element2_19', 'Element2_20']\n",
      "Common Elements: set()\n",
      "\n",
      "Iteration 5:\n",
      "List1 Chunk (10): ['Element1_41', 'Element1_42', 'Element1_43', 'Element1_44', 'Element1_45', 'Element1_46', 'Element1_47', 'Element1_48', 'Element1_49', 'Element1_50']\n",
      "List2 Chunk (10): ['Element2_1', 'Element2_2', 'Element2_3', 'Element2_4', 'Element2_5', 'Element2_6', 'Element2_7', 'Element2_8', 'Element2_9', 'Element2_10']\n",
      "Common Elements: set()\n",
      "\n",
      "Iteration 6:\n",
      "List1 Chunk (10): ['Element1_51', 'Element1_52', 'Element1_53', 'Element1_54', 'Element1_55', 'Element1_56', 'Element1_57', 'Element1_58', 'Element1_59', 'Element1_60']\n",
      "List2 Chunk (10): ['Element2_11', 'Element2_12', 'Element2_13', 'Element2_14', 'Element2_15', 'Element2_16', 'Element2_17', 'Element2_18', 'Element2_19', 'Element2_20']\n",
      "Common Elements: set()\n",
      "\n",
      "Iteration 7:\n",
      "List1 Chunk (10): ['Element1_61', 'Element1_62', 'Element1_63', 'Element1_64', 'Element1_65', 'Element1_66', 'Element1_67', 'Element1_68', 'Element1_69', 'Element1_70']\n",
      "List2 Chunk (10): ['Element2_1', 'Element2_2', 'Element2_3', 'Element2_4', 'Element2_5', 'Element2_6', 'Element2_7', 'Element2_8', 'Element2_9', 'Element2_10']\n",
      "Common Elements: set()\n",
      "\n",
      "Iteration 8:\n",
      "List1 Chunk (10): ['Element1_71', 'Element1_72', 'Element1_73', 'Element1_74', 'Element1_75', 'Element1_76', 'Element1_77', 'Element1_78', 'Element1_79', 'Element1_80']\n",
      "List2 Chunk (10): ['Element2_11', 'Element2_12', 'Element2_13', 'Element2_14', 'Element2_15', 'Element2_16', 'Element2_17', 'Element2_18', 'Element2_19', 'Element2_20']\n",
      "Common Elements: set()\n",
      "\n",
      "Iteration 9:\n",
      "List1 Chunk (10): ['Element1_81', 'Element1_82', 'Element1_83', 'Element1_84', 'Element1_85', 'Element1_86', 'Element1_87', 'Element1_88', 'Element1_89', 'Element1_90']\n",
      "List2 Chunk (10): ['Element2_1', 'Element2_2', 'Element2_3', 'Element2_4', 'Element2_5', 'Element2_6', 'Element2_7', 'Element2_8', 'Element2_9', 'Element2_10']\n",
      "Common Elements: set()\n",
      "\n",
      "Iteration 10:\n",
      "List1 Chunk (7): ['Element1_91', 'Element1_92', 'Element1_93', 'Element1_94', 'Element1_95', 'Element1_96', 'Element1_97']\n",
      "List2 Chunk (7): ['Element2_11', 'Element2_12', 'Element2_13', 'Element2_14', 'Element2_15', 'Element2_16', 'Element2_17']\n",
      "Common Elements: set()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "from typing import List, Tuple, Iterator\n",
    "\n",
    "def chunked_iterable(iterable: List, chunk_size: int) -> Iterator[List]:\n",
    "    \"\"\"\n",
    "    Yield successive chunks of size `chunk_size` from `iterable`.\n",
    "    \n",
    "    Args:\n",
    "        iterable (List): The list to be divided into chunks.\n",
    "        chunk_size (int): The size of each chunk.\n",
    "    \n",
    "    Yields:\n",
    "        List: Chunks of the original list.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(iterable), chunk_size):\n",
    "        yield iterable[i:i + chunk_size]\n",
    "\n",
    "def iterate_lists(\n",
    "    list1: List,\n",
    "    list2: List,\n",
    "    chunk_size: int\n",
    ") -> Iterator[Tuple[List, List]]:\n",
    "    \"\"\"\n",
    "    Iterate through list1 and list2 in chunks. When list2 is exhausted,\n",
    "    restart it. If the last chunk of list1 is smaller than chunk_size,\n",
    "    list2's chunk will match this smaller size.\n",
    "    \n",
    "    Args:\n",
    "        list1 (List): The larger list to iterate through.\n",
    "        list2 (List): The smaller list to cycle through.\n",
    "        chunk_size (int): The size of each chunk.\n",
    "    \n",
    "    Yields:\n",
    "        Tuple[List, List]: A tuple containing a chunk from list1 and a corresponding chunk from list2.\n",
    "    \"\"\"\n",
    "    list1_chunks = chunked_iterable(list1, chunk_size)\n",
    "    list2_chunks = list(chunked_iterable(list2, chunk_size))\n",
    "    \n",
    "    if not list2_chunks:\n",
    "        raise ValueError(\"list2 must contain at least one element.\")\n",
    "    \n",
    "    # Create a cycling iterator for list2 chunks\n",
    "    list2_cycle = itertools.cycle(list2_chunks)\n",
    "    \n",
    "    for chunk1 in list1_chunks:\n",
    "        current_chunk_size = len(chunk1)\n",
    "        \n",
    "        # Get the next chunk from list2_cycle\n",
    "        chunk2_full = next(list2_cycle)\n",
    "        \n",
    "        # If the current chunk size is less than chunk_size, adjust chunk2 accordingly\n",
    "        if current_chunk_size < chunk_size:\n",
    "            if current_chunk_size > len(chunk2_full):\n",
    "                # If chunk2_full is smaller than current_chunk_size, adjust accordingly\n",
    "                # This can happen if list2 is smaller than chunk_size\n",
    "                # Cycle through list2_chunks to accumulate enough elements\n",
    "                needed = current_chunk_size\n",
    "                chunk2 = []\n",
    "                while needed > 0:\n",
    "                    next_chunk = next(list2_cycle)\n",
    "                    if len(next_chunk) <= needed:\n",
    "                        chunk2.extend(next_chunk)\n",
    "                        needed -= len(next_chunk)\n",
    "                    else:\n",
    "                        chunk2.extend(next_chunk[:needed])\n",
    "                        # Adjust the cycle iterator to include the remaining elements\n",
    "                        # Not straightforward with itertools.cycle, so handle differently\n",
    "                        # Here, we'll not handle overlapping chunks, just take as much as possible\n",
    "                        needed = 0\n",
    "                # Truncate to the exact needed size\n",
    "                chunk2 = chunk2[:current_chunk_size]\n",
    "            else:\n",
    "                # If list2's chunk is large enough, slice it\n",
    "                chunk2 = chunk2_full[:current_chunk_size]\n",
    "        else:\n",
    "            chunk2 = chunk2_full\n",
    "        \n",
    "        yield (chunk1, chunk2)\n",
    "\n",
    "def main():\n",
    "    # Example lists\n",
    "    list1 = [f\"Element1_{i}\" for i in range(1, 98)]  # 97 elements\n",
    "    list2 = [f\"Element2_{i}\" for i in range(1, 21)]   # 20 elements\n",
    "    \n",
    "    chunk_size = 10  # Define the size of each chunk\n",
    "    \n",
    "    # Initialize the iterator\n",
    "    paired_chunks = iterate_lists(list1, list2, chunk_size)\n",
    "    \n",
    "    # Iterate and perform comparisons\n",
    "    for idx, (chunk1, chunk2) in enumerate(paired_chunks, 1):\n",
    "        print(f\"Iteration {idx}:\")\n",
    "        print(f\"List1 Chunk ({len(chunk1)}): {chunk1}\")\n",
    "        print(f\"List2 Chunk ({len(chunk2)}): {chunk2}\")\n",
    "        \n",
    "        # Example comparison: Count common elements\n",
    "        common_elements = set(chunk1) & set(chunk2)\n",
    "        print(f\"Common Elements: {common_elements}\\n\")\n",
    "        \n",
    "        # Replace the above with your actual comparison logic\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161627, 9515, 17958, 1057)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_0), len(train_1), len(test_0), len(test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = model_Transformer.tokenizer(train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=203, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetTokenizerFast(name_or_path='sentence-transformers/multi-qa-mpnet-base-dot-v1', vocab_size=30527, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t104: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30526: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_Transformer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorDataset, DataLoader\n\u001b[0;32m----> 3\u001b[0m dataset_0 \u001b[38;5;241m=\u001b[39m \u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m loader_0 \u001b[38;5;241m=\u001b[39m DataLoader(dataset_0, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/scratch/xj2173/Heart-Disease-Prediction-LLM-Hackathon/.venv/lib64/python3.9/site-packages/torch/utils/data/dataset.py:205\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "File \u001b[0;32m/scratch/xj2173/Heart-Disease-Prediction-LLM-Hackathon/.venv/lib64/python3.9/site-packages/torch/utils/data/dataset.py:206\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m--> 206\u001b[0m         \u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors\n\u001b[1;32m    207\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset_0 = TensorDataset([(x, y) for x, y in zip(train_0, [0]*len(train_0))])\n",
    "loader_0 = DataLoader(dataset_0, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import InputExample, losses\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Transformer = SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Logistic Regression Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "features, labels = batch(1000)\n",
    "X = model_Transformer.encode(features)\n",
    "y = np.array(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.72      0.73       198\n",
      "         1.0       0.73      0.74      0.73       202\n",
      "\n",
      "    accuracy                           0.73       400\n",
      "   macro avg       0.73      0.73      0.73       400\n",
      "weighted avg       0.73      0.73      0.73       400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/xj2173/Heart-Disease-Prediction-LLM-Hackathon/.venv/lib64/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets\n",
    "features, labels = batch(2000)\n",
    "X_train, y_train = model_Transformer.encode(features), np.array(labels)  \n",
    "X_test,  y_test  = model_Transformer.encode()\n",
    "\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "model = LogisticRegression(penalty=None)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
